{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the performance benefits of the Cachetools library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import udf, avg, col,lit,call_udf,min,call_builtin,call_function,call_udf\n",
    "from snowflake.snowpark.types import IntegerType, FloatType, StringType, BooleanType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('credentials.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pickle File\n",
    "Run the following code to build the sample pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "day_dict = {1: 'monday', 2: 'tuesday', 3: 'wednesday', 4: 'thursday', 5: 'friday', 6: 'saturday', 7: 'sunday'}\n",
    "print(day_dict)\n",
    "\n",
    "with open('alldays.pkl', 'wb') as file:\n",
    "    pickle.dump(day_dict, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's test the pickle file output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function to test loading pickle file\n",
    "def getname():\n",
    "    with open('alldays.pkl','rb') as fileopen:\n",
    "        f1=pickle.load(fileopen)               \n",
    "    return f1\n",
    "\n",
    "def getday(key):\n",
    "    dict1=getname()\n",
    "    return dict1[key]\n",
    "\n",
    "r=getday(3)\n",
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating named Stage and Uploading the Pickle File\n",
    "\n",
    "Next, we will create a stage in Snowflake to upload the pickle file to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"create or replace stage pythonstage\").collect()\n",
    "session.file.put(\"alldays.pkl\", \"pythonstage\", auto_compress=False,overwrite=True)\n",
    "session.sql(\"ls @pythonstage\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pickle file successfully loaded, we are now ready to A/B test our UDF with and without Cachetools in the next steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Python UDF Without Cachetools\n",
    "\n",
    "The block of code below creates a Python UDF without any use of the Cachetools library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cachetools\n",
    "from snowflake.snowpark.types import StringType,IntegerType\n",
    "\n",
    "'''\n",
    "We are importing the Pickle file in the session.\n",
    "\n",
    "extract_name() -> deserializes the dictionary from the pickle and returns a dictionary object to the caller\n",
    "\n",
    "getvalue() -> Takes a day number and return the name.\n",
    "\n",
    "'''\n",
    "session.add_import(\"@pythonstage/alldays.pkl\")\n",
    "def extract_name()->dict:\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    file_path = import_dir + \"alldays.pkl\"\n",
    "    \n",
    "    with open(file_path,'rb') as file:\n",
    "        dict1=pickle.load(file)\n",
    "\n",
    "    return dict1\n",
    "\n",
    "def getvalue(key:int)->str:   \n",
    "    filedict= extract_name()\n",
    "    return filedict[key]\n",
    "    \n",
    "# Creating a Python UDF\n",
    "udf_nocache = session.udf.register(\n",
    "    func=getvalue,\n",
    "    name=\"udf_nocachetools\",\n",
    "    stage_location='pythonstage',\n",
    "    is_permanent=True,\n",
    "    replace=True, \n",
    "    input_types=[IntegerType()],\n",
    "    return_type=StringType()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample data\n",
    "Next, we will create 2 million rows of sample data for the UDF to run against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_random = np.random.randint(low=1, high=7, size=(2000000,3))\n",
    "df = pd.DataFrame(arr_random, columns=['invoice_num','trx_num','weekday'])\n",
    "\n",
    "df_transactions=session.createDataFrame(df,schema=['invoice_num','trx_num','weekday'])\n",
    "\n",
    "df_transactions.count() # 2 Million records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the UDF\n",
    "Let's call the UDF and create a new table from the resultset with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "st=datetime.now()\n",
    "df_transactions.withColumn('weekdayname',call_udf('udf_nocachetools',df_transactions['\"weekday\"'].astype('int')))\\\n",
    ".write.mode('overwrite').save_as_table(\"NoCacheTransactionTable\")\n",
    "et=datetime.now()\n",
    "print(f\"Total duration without using Cachetools library ={(et-st).total_seconds()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, confirm that the UDF ran and successfully created a new table as expected by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"select * from NoCacheTransactionTable limit 10\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Python User Defined Function With Cachetools\n",
    "\n",
    "In the below cell, we will leverage the [Cachetools](https://pypi.org/project/cachetools/) library which will read the pickle file once and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cachetools\n",
    "from snowflake.snowpark.types import StringType\n",
    "import zipfile\n",
    "\n",
    "'''\n",
    "Using cachetools decorator while creating the function extract_name. Using this decorator the file will be read once and then cached. \n",
    "Other UDF execution will use the cached file and avoids reading the file from the storage.\n",
    "'''\n",
    "\n",
    "session.add_import(\"@pythonstage/alldays.pkl\")\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def extract_name()->dict:\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    file_path = import_dir + \"alldays.pkl\"\n",
    "    \n",
    "    with open(file_path,'rb') as file:\n",
    "        dict1=pickle.load(file)\n",
    "\n",
    "    return dict1\n",
    "\n",
    "\n",
    "def getvalue(key:int)->str:   \n",
    "    filedict= extract_name()\n",
    "    return filedict[key]\n",
    "    \n",
    "\n",
    "session.add_packages(\"cachetools\")\n",
    "udf_cache = session.udf.register(\n",
    "    func=getvalue,\n",
    "    name=\"udf_withcachetools\",\n",
    "    stage_location='pythonstage',\n",
    "    is_permanent=True, \n",
    "    replace=True, \n",
    "    input_types=[IntegerType()],\n",
    "    return_type=StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample data\n",
    "\n",
    "Just like in the previous test, we will create 2 million rows of sample data for the UDF to run against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_random = np.random.randint(low=1, high=7, size=(2000000,3))\n",
    "df = pd.DataFrame(arr_random, columns=['invoice_num','trx_num','weekday'])\n",
    "\n",
    "df_transactions=session.createDataFrame(df,schema=['invoice_num','trx_num','weekday'])\n",
    "\n",
    "df_transactions.count() # 2 Million records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the UDF\n",
    "\n",
    "Let's call the UDF and create a new table from the resultset with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "st=datetime.now()\n",
    "df_transactions.withColumn('\"weekdayname\"',call_udf('udf_withcachetools',df_transactions['\"weekday\"'].astype('int')))\\\n",
    ".write.mode('overwrite').save_as_table(\"CacheToolsTransactionTable\")\n",
    "et=datetime.now()\n",
    "print(f\"Total duration ={(et-st).total_seconds()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Use cachetools library in UDFs and SPs, wherever you read the files and store them in the memory.</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysnowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9434343805f944cca68c9baa90b0479ced9875b0751064f0bea749b2255bd0b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
